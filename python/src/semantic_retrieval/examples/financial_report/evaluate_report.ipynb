{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fd277b6-2bc6-4cdf-8d24-69a56d95f8e3",
   "metadata": {},
   "source": [
    "# Evaluate SR output - hallucations and faithfullness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94b6e8fa-c4ea-416f-b874-bb79e65c02ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path = ['/Users/jonathan/Projects/semantic-retrieval/python/src'] + sys.path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48a1afb-45f3-4e90-8767-8fd720187f14",
   "metadata": {},
   "source": [
    "## Reproduce reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa7678c1-aded-4f5d-8b99-48a706acc3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: can't open file '/Users/jonathan/Projects/semantic-retrieval/python/src/semantic_retrieval/examples/financial_report/python/src/semantic_retrieval/examples/financial_report/generate_report.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# in semantic-retrieval/\n",
    "# Reproduce net income report\n",
    "!python python/src/semantic_retrieval/examples/financial_report/generate_report.py --retrieval-query=\"annual net income 2022\" --data-extraction-prompt=\"annual net income in 2022\" --overfetch-factor=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0345000-9feb-41b7-8bca-1341fee7ee2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: can't open file '/Users/jonathan/Projects/semantic-retrieval/python/src/semantic_retrieval/examples/financial_report/python/src/semantic_retrieval/examples/financial_report/generate_report.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# Reproduce covid report\n",
    "# in semantic-retrieval/\n",
    "!python python/src/semantic_retrieval/examples/financial_report/generate_report.py --retrieval-query=\"covid 19 impact\" --data-extraction-prompt=\"covid 19 impact\" --overfetch-factor=20 --log-level=DEBUG --client-name=client_a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e127aa22-f68d-4a46-aa4d-df412cd64bff",
   "metadata": {},
   "source": [
    "## Example eval input: \n",
    "* Client portfolio database (local CSV)\n",
    "* Intermediate SR results: raw retrieved document chunks\n",
    "* SR final output saved in local files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d423fb10-ec9b-4614-b9f6-09a7cff65f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "from semantic_retrieval.retrieval.csv_retriever import CSVRetriever\n",
    "# from semantic_retrieval.common import types\n",
    "# from semantic_retrieval.evaluation.lib import (\n",
    "#     LocalFileSystemGenLLMEvalDataset,\n",
    "#     evaluate_llm_eval_dataset_local_filesystem,\n",
    "#     evaluate_sample_local_filesystem,\n",
    "#     local_filesystem_dataset_to_df,\n",
    "#     # LocalFileSystemNumericalEvalDatasetConfig,\n",
    "#     LocalFileSystemIDSetPairEvalDatasetConfig,\n",
    "#     file_contents,\n",
    "#     # IDSetPairEvalDataPathMuncher,\n",
    "#     IDSetPairEvalDataset,\n",
    "# )\n",
    "\n",
    "import json\n",
    "\n",
    "from semantic_retrieval.evaluation.metrics import jaccard_similarity\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "\n",
    "ROOT_DATA_DIR = \"../../../../../examples/example_data/financial_report/\"\n",
    "\n",
    "print(\"Ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0325ef1f-123a-4b7e-a52a-0c8d544254fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test case:\n",
      " ('net_income_vs_retrieved', <function test_1_2_data_muncher at 0x177d6c860>, 'artifacts/raw_retrieved_chunks_10k_net_income.json', 'artifacts/portfolio_10k_net_income_report.txt')\n",
      "\n",
      "Test case:\n",
      " ('covid_vs_retrieved', <function test_1_2_data_muncher at 0x177d6c860>, 'artifacts/raw_retrieved_chunks_10k_covid.json', 'artifacts/portfolio_10k_covid_report.txt')\n",
      "\n",
      "Test case:\n",
      " ('net_income_e2e', <function test_3_4_data_muncher at 0x177d6c9a0>, 'portfolios/sarmad_portfolio.csv', 'artifacts/portfolio_10k_net_income_report.txt')\n",
      "\n",
      "Test case:\n",
      " ('covid_e2e', <function test_3_4_data_muncher at 0x177d6c9a0>, 'portfolios/sarmad_portfolio.csv', 'artifacts/portfolio_10k_covid_report.txt')\n"
     ]
    }
   ],
   "source": [
    "from semantic_retrieval.examples.financial_report.eval_test_config import TEST_CASES\n",
    "\n",
    "for tc in TEST_CASES:\n",
    "    print(\"\\nTest case:\\n\", tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1209104-2066-4832-8cab-b7971784be14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structured eval params:\n",
      "\n",
      "\n",
      "SampleEvaluationParams:\n",
      "\tself.output_sample={'JNJ', 'NVDA', 'UNH', 'AMZN', 'AAPL', 'PEP', 'MRK'}\n",
      "\tself.evaluation_fn=functools.partial(<function idset_jaccard_with_gt at 0x1770393a0>, gt={'JNJ', 'NVDA', 'UNH', 'AMZN', 'AAPL', 'PEP', 'MRK'}, name='net_income_vs_retrieved')\n",
      "\n",
      "SampleEvaluationParams:\n",
      "\tself.output_sample={'JNJ', 'NVDA', 'UNH', 'AMZN', 'AAPL', 'PEP', 'MRK'}\n",
      "\tself.evaluation_fn=functools.partial(<function idset_jaccard_with_gt at 0x1770393a0>, gt={'JNJ', 'NVDA', 'UNH', 'AMZN', 'AAPL', 'PEP', 'MRK'}, name='covid_vs_retrieved')\n",
      "\n",
      "SampleEvaluationParams:\n",
      "\tself.output_sample={'JNJ', 'NVDA', 'UNH', 'AMZN', 'AAPL', 'PEP', 'MRK'}\n",
      "\tself.evaluation_fn=functools.partial(<function idset_jaccard_with_gt at 0x1770393a0>, gt={'JNJ', 'NVDA', 'UNH', 'AMZN', 'AAPL', 'PEP', 'MRK'}, name='net_income_e2e')\n",
      "\n",
      "SampleEvaluationParams:\n",
      "\tself.output_sample={'JNJ', 'NVDA', 'UNH', 'AMZN', 'AAPL', 'PEP', 'MRK'}\n",
      "\tself.evaluation_fn=functools.partial(<function idset_jaccard_with_gt at 0x1770393a0>, gt={'JNJ', 'NVDA', 'UNH', 'AMZN', 'AAPL', 'PEP', 'MRK'}, name='covid_e2e')\n",
      "\n",
      "\n",
      "\n",
      "Evaluation params with errors (empty list is good): []\n"
     ]
    }
   ],
   "source": [
    "from semantic_retrieval.functional.functional import result_reduce_list_separate\n",
    "\n",
    "from semantic_retrieval.examples.financial_report import lib as fr_lib\n",
    "\n",
    "evaluation_params_list = [\n",
    "    await fr_lib.test_case_to_sample_eval_params(tc, ROOT_DATA_DIR) for tc in TEST_CASES\n",
    "]\n",
    "\n",
    "print(\"Structured eval params:\\n\")\n",
    "\n",
    "# Separate out the valid test params from the error ones.\n",
    "evaluation_params_valid, evaluation_params_errs = result_reduce_list_separate(\n",
    "    evaluation_params_list\n",
    ")\n",
    "\n",
    "for eval_params in evaluation_params_valid:\n",
    "    print(eval_params)\n",
    "\n",
    "\n",
    "print(f\"\\n\\n\\nEvaluation params with errors (empty list is good): {evaluation_params_errs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ca1a7c7-01fe-43de-995e-b82a0596a56a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th>interpretation</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>net_income_vs_retrieved</th>\n",
       "      <th>Jaccard similarity with ground truth</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>covid_vs_retrieved</th>\n",
       "      <th>Jaccard similarity with ground truth</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>net_income_e2e</th>\n",
       "      <th>Jaccard similarity with ground truth</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>covid_e2e</th>\n",
       "      <th>Jaccard similarity with ground truth</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                              value\n",
       "name                    interpretation                             \n",
       "net_income_vs_retrieved Jaccard similarity with ground truth    1.0\n",
       "covid_vs_retrieved      Jaccard similarity with ground truth    1.0\n",
       "net_income_e2e          Jaccard similarity with ground truth    1.0\n",
       "covid_e2e               Jaccard similarity with ground truth    1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Ok(None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from semantic_retrieval.evaluation.lib import (\n",
    "    eval_res_to_df,\n",
    "    evaluate,\n",
    ")\n",
    "\n",
    "# Helper function to pretty print all the results\n",
    "def _print(df: pd.DataFrame) -> None:\n",
    "    return display(df.set_index([\"name\", \"interpretation\"]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Run `evaluate()` on the valid params.\n",
    "eval_res = evaluate(evaluation_params_valid)\n",
    "\n",
    "# Print results\n",
    "df_eval_res = eval_res.map(eval_res_to_df)\n",
    "df_eval_res.map(_print)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semantic_retrieval",
   "language": "python",
   "name": "semantic_retrieval"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
